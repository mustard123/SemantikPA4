{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import fbeta_score, make_scorer\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.model_selection import cross_val_score, StratifiedKFold, KFold\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from nltk.corpus import stopwords\n",
    "import numpy as np\n",
    "\n",
    "from pa4_functions import load_data, print_stats, ExractSimpleFeatures, extractSegments, evaluateCV, evaluateCV_check, printLabelsToFile, printNMostInformative\n",
    "\n",
    "def useClassifier(clf, encoder, train_features, train_labels, test_features, filename):\n",
    "    avg = evaluateCV(clf, encoder, train_features, train_labels)\n",
    "    print(\"Average: %.2f\" % avg)\n",
    "    evaluateCV_check(clf, train_features, train_labels)\n",
    "\n",
    "    clf.fit(train_features, train_labels)   \n",
    "    test_label_predicted = clf.predict(test_features)\n",
    "\n",
    "    printLabelsToFile(encoder, test_label_predicted, filename)\n",
    "    print(\"printed to \" + '\"' + filename + '\"')\n",
    "###########################################################################################\n",
    "# 2. LOAD DATA\n",
    "###########################################################################################\n",
    "train_data, train_labels = load_data('train.json.txt', verbose=False)\n",
    "test_data, _ = load_data('test.json.txt', verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gridSearchCV(pipeline, parameters, features, labels):\n",
    "    f_scorer = make_scorer(fbeta_score, beta=0.5, average='macro')\n",
    "    gridSearch = GridSearchCV(pipeline, parameters, scoring=f_scorer, cv=5, verbose=2)\n",
    "    gridSearch.fit(features, labels)\n",
    "    means = gridSearch.cv_results_['mean_test_score']\n",
    "    stds = gridSearch.cv_results_['std_test_score']\n",
    "    parameters = gridSearch.cv_results_['params']\n",
    "    print(\"Best: \")\n",
    "    print(\"%0.3f (+/-%0.03f) for %r\" % (means[gridSearch.best_index_], \n",
    "                                        stds[gridSearch.best_index_] * 2, parameters[gridSearch.best_index_]))\n",
    "    print(\"\\nGrid: \")\n",
    "    for mean, std, params in zip(means, stds, parameters):\n",
    "        print(\"%0.3f (+/-%0.03f) for %r\"\n",
    "              % (mean, std * 2, params))\n",
    "    return gridSearch.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "###########################################################################################\n",
    "# 2. EXTRACT FEATURES and LABELS\n",
    "###########################################################################################\n",
    "# LABELS\n",
    "le = LabelEncoder()\n",
    "train_labels_onehot =le.fit_transform(train_labels)\n",
    "\n",
    "# FEATURES - TRAIN & TEST\n",
    "train_data_featurized = ExractSimpleFeatures(train_data, verbose=False)\n",
    "test_data_featurized = ExractSimpleFeatures(test_data, verbose=False)\n",
    "\n",
    "train_data_middle_segment = extractSegments(train_data, False, True, False)\n",
    "test_data_middle_segment = extractSegments(test_data, False, True, False)\n",
    "\n",
    "train_data_all_segments = extractSegments(train_data, True, True, True)\n",
    "test_data_all_segments = extractSegments(test_data, True, True, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "###########################################################################################\n",
    "# 3. BUILD PIPELINES\n",
    "###########################################################################################\n",
    "#clf = make_pipeline(DictVectorizer(), LogisticRegression())\n",
    "clf2 = make_pipeline(TfidfVectorizer(), LogisticRegression())\n",
    "clf3 = make_pipeline(TfidfVectorizer(ngram_range=(0, 3), analyzer='char'), LogisticRegression())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Cross-validation scores (StratifiedKFold):  [0.58280724 0.59586451 0.57539648 0.57802525 0.57415612]\n",
      "Mean cv score (StratifiedKFold):  0.5812499186909281\n"
     ]
    }
   ],
   "source": [
    "# can be optimized with parameters\n",
    "clf = make_pipeline(DictVectorizer(), LogisticRegression(C=1000, multi_class='multinomial', solver='newton-cg'))\n",
    "evaluateCV_check(clf, train_data_featurized, train_labels_onehot) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters0 = {\n",
    "    'tfidfvectorizer__analyzer': ['word', 'char'],\n",
    "    'tfidfvectorizer__stop_words': ['english', None],\n",
    "    'tfidfvectorizer__ngram_range': [(1, 1), (2, 2), (3, 3)],\n",
    "    #'tfidfvectorizer__use_idf': [True, False],\n",
    "    #'tfidfvectorizer__max_df': [0.1, 0.5, 1.0],\n",
    "    #'tfidfvectorizer__min_df': [1, 5, 10],    \n",
    "}\n",
    "parameters1 = {\n",
    "    'logisticregression__C': [1, 10, 100, 1000],\n",
    "    'logisticregression__penalty':['l1', 'l2'],\n",
    "    'logisticregression__solver':['liblinear', 'saga'],\n",
    "    #'logisticregression__dual':[True, False], # can only be true for l2 penalty with liblinear solver\n",
    "}\n",
    "parameters2 = {\n",
    "    'logisticregression__C': [1, 10, 100, 1000],\n",
    "    'logisticregression__penalty':['l2'],\n",
    "    'logisticregression__solver':['lbfgs', 'newton-cg', 'sag'], # only with l2 penalty\n",
    "    'logisticregression__multi_class':['ovr', 'multinomial'], # multinomial not for solver=’liblinear’\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 12 candidates, totalling 60 fits\n",
      "[CV] tfidfvectorizer__analyzer=word, tfidfvectorizer__ngram_range=(1, 1), tfidfvectorizer__stop_words=english \n",
      "[CV]  tfidfvectorizer__analyzer=word, tfidfvectorizer__ngram_range=(1, 1), tfidfvectorizer__stop_words=english, total=   0.7s\n",
      "[CV] tfidfvectorizer__analyzer=word, tfidfvectorizer__ngram_range=(1, 1), tfidfvectorizer__stop_words=english \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    1.1s remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  tfidfvectorizer__analyzer=word, tfidfvectorizer__ngram_range=(1, 1), tfidfvectorizer__stop_words=english, total=   0.8s\n",
      "[CV] tfidfvectorizer__analyzer=word, tfidfvectorizer__ngram_range=(1, 1), tfidfvectorizer__stop_words=english \n",
      "[CV]  tfidfvectorizer__analyzer=word, tfidfvectorizer__ngram_range=(1, 1), tfidfvectorizer__stop_words=english, total=   0.8s\n",
      "[CV] tfidfvectorizer__analyzer=word, tfidfvectorizer__ngram_range=(1, 1), tfidfvectorizer__stop_words=english \n",
      "[CV]  tfidfvectorizer__analyzer=word, tfidfvectorizer__ngram_range=(1, 1), tfidfvectorizer__stop_words=english, total=   0.6s\n",
      "[CV] tfidfvectorizer__analyzer=word, tfidfvectorizer__ngram_range=(1, 1), tfidfvectorizer__stop_words=english \n",
      "[CV]  tfidfvectorizer__analyzer=word, tfidfvectorizer__ngram_range=(1, 1), tfidfvectorizer__stop_words=english, total=   0.8s\n",
      "[CV] tfidfvectorizer__analyzer=word, tfidfvectorizer__ngram_range=(1, 1), tfidfvectorizer__stop_words=None \n",
      "[CV]  tfidfvectorizer__analyzer=word, tfidfvectorizer__ngram_range=(1, 1), tfidfvectorizer__stop_words=None, total=   0.9s\n",
      "[CV] tfidfvectorizer__analyzer=word, tfidfvectorizer__ngram_range=(1, 1), tfidfvectorizer__stop_words=None \n",
      "[CV]  tfidfvectorizer__analyzer=word, tfidfvectorizer__ngram_range=(1, 1), tfidfvectorizer__stop_words=None, total=   0.8s\n",
      "[CV] tfidfvectorizer__analyzer=word, tfidfvectorizer__ngram_range=(1, 1), tfidfvectorizer__stop_words=None \n",
      "[CV]  tfidfvectorizer__analyzer=word, tfidfvectorizer__ngram_range=(1, 1), tfidfvectorizer__stop_words=None, total=   0.9s\n",
      "[CV] tfidfvectorizer__analyzer=word, tfidfvectorizer__ngram_range=(1, 1), tfidfvectorizer__stop_words=None \n",
      "[CV]  tfidfvectorizer__analyzer=word, tfidfvectorizer__ngram_range=(1, 1), tfidfvectorizer__stop_words=None, total=   0.7s\n",
      "[CV] tfidfvectorizer__analyzer=word, tfidfvectorizer__ngram_range=(1, 1), tfidfvectorizer__stop_words=None \n",
      "[CV]  tfidfvectorizer__analyzer=word, tfidfvectorizer__ngram_range=(1, 1), tfidfvectorizer__stop_words=None, total=   0.9s\n",
      "[CV] tfidfvectorizer__analyzer=word, tfidfvectorizer__ngram_range=(2, 2), tfidfvectorizer__stop_words=english \n",
      "[CV]  tfidfvectorizer__analyzer=word, tfidfvectorizer__ngram_range=(2, 2), tfidfvectorizer__stop_words=english, total=   0.8s\n",
      "[CV] tfidfvectorizer__analyzer=word, tfidfvectorizer__ngram_range=(2, 2), tfidfvectorizer__stop_words=english \n",
      "[CV]  tfidfvectorizer__analyzer=word, tfidfvectorizer__ngram_range=(2, 2), tfidfvectorizer__stop_words=english, total=   0.8s\n",
      "[CV] tfidfvectorizer__analyzer=word, tfidfvectorizer__ngram_range=(2, 2), tfidfvectorizer__stop_words=english \n",
      "[CV]  tfidfvectorizer__analyzer=word, tfidfvectorizer__ngram_range=(2, 2), tfidfvectorizer__stop_words=english, total=   0.8s\n",
      "[CV] tfidfvectorizer__analyzer=word, tfidfvectorizer__ngram_range=(2, 2), tfidfvectorizer__stop_words=english \n",
      "[CV]  tfidfvectorizer__analyzer=word, tfidfvectorizer__ngram_range=(2, 2), tfidfvectorizer__stop_words=english, total=   0.8s\n",
      "[CV] tfidfvectorizer__analyzer=word, tfidfvectorizer__ngram_range=(2, 2), tfidfvectorizer__stop_words=english \n",
      "[CV]  tfidfvectorizer__analyzer=word, tfidfvectorizer__ngram_range=(2, 2), tfidfvectorizer__stop_words=english, total=   0.7s\n",
      "[CV] tfidfvectorizer__analyzer=word, tfidfvectorizer__ngram_range=(2, 2), tfidfvectorizer__stop_words=None \n",
      "[CV]  tfidfvectorizer__analyzer=word, tfidfvectorizer__ngram_range=(2, 2), tfidfvectorizer__stop_words=None, total=   1.2s\n",
      "[CV] tfidfvectorizer__analyzer=word, tfidfvectorizer__ngram_range=(2, 2), tfidfvectorizer__stop_words=None \n",
      "[CV]  tfidfvectorizer__analyzer=word, tfidfvectorizer__ngram_range=(2, 2), tfidfvectorizer__stop_words=None, total=   1.0s\n",
      "[CV] tfidfvectorizer__analyzer=word, tfidfvectorizer__ngram_range=(2, 2), tfidfvectorizer__stop_words=None \n"
     ]
    }
   ],
   "source": [
    "gridSearchCV(clf2, parameters0, train_data_middle_segment, train_labels_onehot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "relation              precision     recall    f-score    support\n",
      "------------------    ---------  ---------  ---------  ---------\n",
      "NO_REL                    0.612      0.705      0.629       2300\n",
      "author                    0.842      0.871      0.848       2653\n",
      "capital                   0.890      0.573      0.800        510\n",
      "has_spouse                0.835      0.850      0.838       3019\n",
      "worked_at                 0.795      0.581      0.740       1178\n",
      "------------------    ---------  ---------  ---------  ---------\n",
      "macro-average             0.795      0.716      0.771       9660\n",
      "Average: 0.77\n",
      "\n",
      "Cross-validation scores (StratifiedKFold):  [0.76231294 0.78004128 0.79387289 0.76638445 0.751862  ]\n",
      "Mean cv score (StratifiedKFold):  0.770894711364053\n",
      "printed to \"test.txt\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n"
     ]
    }
   ],
   "source": [
    "###########################################################################################\n",
    "# 4. USE PIPELINES\n",
    "###########################################################################################\n",
    "useClassifier(clf3, le, train_data_middle_segment, train_labels_onehot, test_data_middle_segment, 'predictions_clf3.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#useClassifier(clf2, le, train_data_middle_segment, train_labels_onehot, test_data_middle_segment, 'predictions_clf2.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top features used to predict: \n",
      "(5, 13485)\n",
      "\n",
      "Class NO_REL best: \n",
      "(2.5744258445318517, ' an')\n",
      "(2.6452725899682488, '. ')\n",
      "(3.849651134579289, ' ,')\n",
      "\n",
      "Class author best: \n",
      "(3.5879475095786724, \"'\")\n",
      "(4.029460877872109, 'y')\n",
      "(4.944191387880969, 'by')\n",
      "\n",
      "Class capital best: \n",
      "(3.138655611011565, 'in')\n",
      "(3.7073144125946293, ',,')\n",
      "(4.438772077639486, ',,,')\n",
      "\n",
      "Class has_spouse best: \n",
      "(4.751422684772724, 'ife')\n",
      "(4.769287527192585, 'wif')\n",
      "(4.864998654459525, ' wi')\n",
      "\n",
      "Class worked_at best: \n",
      "(3.282106103205331, 'at')\n",
      "(3.478830411723698, 'und')\n",
      "(4.428550603474797, ' at')\n"
     ]
    }
   ],
   "source": [
    "print(\"Top features used to predict: \")\n",
    "#printNMostInformative(clf, le, 3, 'dictvectorizer')\n",
    "printNMostInformative(clf3, le, 3, 'tfidfvectorizer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
